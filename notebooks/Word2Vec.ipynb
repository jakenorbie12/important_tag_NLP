{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "import nltk\n",
    "path = get_tmpfile('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Dataset_08-29-2019.txt'\n",
    "df = pd.read_csv(filename, sep = '\\t', encoding = 'unicode_escape')\n",
    "word_data = df['Word'].values.tolist()\n",
    "#word_vec = [nltk.word_tokenize(title) for title in word_data]\n",
    "size = len(word_data) \n",
    "idx_list = [idx + 1 for idx, val in\n",
    "            enumerate(word_data) if val == '.'] \n",
    "res = [word_data[i: j] for i, j in\n",
    "        zip([0] + idx_list, idx_list + \n",
    "        ([size] if idx_list[-1] != size else []))] \n",
    "model = Word2Vec(res, size=24, window=5, min_count=0, workers=4)\n",
    "model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Array2Num(x, array):\n",
    "    return array.index(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaken\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "wv = KeyedVectors.load('word2vec.model')\n",
    "for i in range(24):\n",
    "    df['WordVector' + str(i)] = df['Word'].apply(lambda x: wv[x][i])\n",
    "tag_array = df.Tag.unique().tolist()\n",
    "df['TagNum'] = df['Tag'].apply(lambda x: Array2Num(x, tag_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_group = df.groupby(['Sentence #'])\n",
    "test_sentences = []\n",
    "test_dfs = []\n",
    "train_dfs = []\n",
    "max_sent = int(df['Sentence #'].max())\n",
    "for i in range(round(max_sent / 4)):\n",
    "    found = False\n",
    "    while found == False:\n",
    "        num = random.randint(1, 2999)\n",
    "        if not num in test_sentences:\n",
    "            test_sentences.append(num)\n",
    "            test_dfs.append(sentences_group.get_group(num))\n",
    "            found = True\n",
    "            \n",
    "test_df = pd.concat(test_dfs)\n",
    "drop_list = test_df['Unnamed: 0'].tolist()\n",
    "train_df = df.copy().drop(drop_list)\n",
    "vectorlist = []\n",
    "for i in range(24):\n",
    "    vectorlist.append('WordVector' + str(i))\n",
    "data_train = train_df[vectorlist].values\n",
    "label_train = train_df['TagNum'].values\n",
    "data_test = test_df[vectorlist].values\n",
    "label_test = test_df['TagNum'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = lgb.Dataset(data_train, label=label_train)\n",
    "params = {}\n",
    "params['objective'] = 'multiclass'\n",
    "params['num_classes'] = 17\n",
    "params['learning_rate'] = 0.03\n",
    "mod = lgb.train(params, d_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9044052077759943\n",
      "[[14322     1     0     3     0     0     2     0     2     0    11     0\n",
      "      1     1     1     1     0]\n",
      " [  256   243     0     0     0     2     1     0     0     0     1     0\n",
      "      0     0     0     1     0]\n",
      " [   83    45   156     0     0     0     0     0     0     0     0     0\n",
      "      0     0     1     0     0]\n",
      " [  178     2     0    85     0     1     1     0     0     0     3     0\n",
      "      0     0     0     0     0]\n",
      " [   54     6     0     0    27     0     4     0     0     1     1     0\n",
      "      0     0     0     0     0]\n",
      " [  211    41    10     0     0    59     9     0     3     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [  174     0     0     0     1     1    67     0     0     4     1     0\n",
      "      0     0     0     0     0]\n",
      " [  104     0     0     0     0     0     0   166     0     0     0     0\n",
      "      4     0     0     0     0]\n",
      " [   13     0     0     0     0     1     0     0     3     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   13     0     0     0     0     0     1     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [  240     0     0     4     0     0     0     0     0     0    76     0\n",
      "      0     0     1     0     0]\n",
      " [    7     0     1     0     4     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   68     1     0     0     0     0     0    13     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [    2     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     1     0     0     0]\n",
      " [    5     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     1     4     0     0]\n",
      " [    6     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     3     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     1]]\n"
     ]
    }
   ],
   "source": [
    "prediction_data = mod.predict(data_test)\n",
    "classed_data = [np.argmax(line) for line in prediction_data]\n",
    "accuracy = accuracy_score(classed_data, label_test)\n",
    "cm = confusion_matrix(label_test, classed_data)\n",
    "print(accuracy)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
