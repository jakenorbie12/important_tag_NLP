{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaken\\Desktop\\Internship\\Jupyter Notebook itNLP\\config\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "ROOT_DIR = os.path.abspath(\"./config\")\n",
    "print(ROOT_DIR)\n",
    "sys.path.append(ROOT_DIR)\n",
    "#import data_config as Dconfig\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "\n",
    "def otherCap(x):\n",
    "    for letter in x:\n",
    "        if letter.isupper():\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def propVow(x):\n",
    "    vowels = 'aeiouAEIOU'\n",
    "    numVow = 0\n",
    "    for letter in x:\n",
    "        if letter in vowels:\n",
    "            numVow += 1\n",
    "    return numVow / len(x)\n",
    "\n",
    "def frontWord(x, array, df):\n",
    "    if x > 0:\n",
    "        return array.index(df['Word'][x-1])\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "def backWord(x, array, df):\n",
    "    if x < len(df.index) - 1:\n",
    "        return array.index(df['Word'][x+1])\n",
    "    else:\n",
    "        return\n",
    "\n",
    "def Tag2Num(x, array):\n",
    "    return array.index(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_gen(filename = 'Dataset_08-29-2019.txt'):\n",
    "    \n",
    "    logging.info('Feature Generation has begun')\n",
    "    df = pd.read_csv(filename, sep='\\t', encoding='unicode_escape')\n",
    "    \n",
    "    df['isFirstCap'] = df['Word'].apply(lambda x: 1 if x[0].isupper() else 0)\n",
    "\n",
    "    df['Length'] = df['Word'].apply(lambda x: len(x))\n",
    "\n",
    "    df['endY'] = df['Word'].apply(lambda x: 1 if x[-1] == 'y' else 0)\n",
    "\n",
    "    df['isNNP'] = df['POS'].apply(lambda x: 1 if x == 'NNP' else 0)\n",
    "\n",
    "    df['isJJ'] = df['POS'].apply(lambda x: 1 if x == 'JJ' else 0)\n",
    "\n",
    "    df['isCD'] = df['POS'].apply(lambda x: 1 if x == 'CD' else 0)\n",
    "\n",
    "    df['otherCap'] = df['Word'].apply(lambda x: otherCap(x))\n",
    "\n",
    "    df['endan'] = df['Word'].apply(lambda x: 1 if x[-2:len(x)] == 'an' else 0)\n",
    "\n",
    "    df['isNum'] = df['Word'].apply(lambda x: 1 if x.isnumeric() else 0)\n",
    "\n",
    "    df['endS'] = df['Word'].apply(lambda x: 1 if x[-1] == 's' else 0)\n",
    "\n",
    "    df['endish'] = df['Word'].apply(lambda x: 1 if x[-3:len(x)] == 'ish' else 0)\n",
    "\n",
    "    df['endese'] = df['Word'].apply(lambda x: 1 if x[-3:len(x)] == 'ese' else 0)\n",
    "\n",
    "    df['propVow'] = df['Word'].apply(lambda x: propVow(x))\n",
    "\n",
    "    logging.info('Simple features have been generated, moving on to difficult features')\n",
    "    tag_array = df.Tag.unique().tolist()\n",
    "    df['TagNum'] = df['Tag'].apply(lambda x: Tag2Num(x, tag_array))\n",
    "    word_array = df.Word.unique().tolist()\n",
    "    df['frontWord'] = df['Unnamed: 0'].apply(lambda x: frontWord(x, word_array, df))\n",
    "\n",
    "    df['backWord'] = df['Unnamed: 0'].apply(lambda x: backWord(x, word_array, df))\n",
    "\n",
    "    logging.info('All features done... saving to file')\n",
    "    \n",
    "    df.to_csv('featured_dataset.csv', encoding = 'unicode-escape')\n",
    "feature_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 9.00000000e+00 0.00000000e+00 ... 3.33333333e-01\n",
      "             nan 1.00000000e+00]\n",
      " [0.00000000e+00 2.00000000e+00 0.00000000e+00 ... 5.00000000e-01\n",
      "  0.00000000e+00 2.00000000e+00]\n",
      " [0.00000000e+00 1.30000000e+01 0.00000000e+00 ... 3.07692308e-01\n",
      "  1.00000000e+00 3.00000000e+00]\n",
      " ...\n",
      " [0.00000000e+00 6.00000000e+00 0.00000000e+00 ... 3.33333333e-01\n",
      "  8.17000000e+02 9.74000000e+02]\n",
      " [0.00000000e+00 4.00000000e+00 0.00000000e+00 ... 2.50000000e-01\n",
      "  7.51000000e+02 2.10000000e+01]\n",
      " [0.00000000e+00 1.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  9.74000000e+02            nan]]\n",
      "[[1.00000000e+00 9.00000000e+00 0.00000000e+00 ... 3.33333333e-01\n",
      "             nan 1.00000000e+00]\n",
      " [0.00000000e+00 2.00000000e+00 0.00000000e+00 ... 5.00000000e-01\n",
      "  0.00000000e+00 2.00000000e+00]\n",
      " [0.00000000e+00 1.30000000e+01 0.00000000e+00 ... 3.07692308e-01\n",
      "  1.00000000e+00 3.00000000e+00]\n",
      " ...\n",
      " [0.00000000e+00 6.00000000e+00 0.00000000e+00 ... 3.33333333e-01\n",
      "  8.17000000e+02 9.74000000e+02]\n",
      " [0.00000000e+00 4.00000000e+00 0.00000000e+00 ... 2.50000000e-01\n",
      "  7.51000000e+02 2.10000000e+01]\n",
      " [0.00000000e+00 1.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  9.74000000e+02            nan]]\n"
     ]
    }
   ],
   "source": [
    "def split(filename = 'featured_dataset.csv'):\n",
    "\n",
    "    df = pd.read_csv(filename, encoding='unicode_escape')\n",
    "    \n",
    "    sentences_group = df.groupby(['Sentence #'])\n",
    "    test_sentences = []\n",
    "    test_dfs = []\n",
    "    train_dfs = []\n",
    "    for i in range(750):\n",
    "        found = False\n",
    "        while found == False:\n",
    "            num = random.randint(1, 2999)\n",
    "            if not num in test_sentences:\n",
    "                test_sentences.append(num)\n",
    "                test_dfs.append(sentences_group.get_group(num))\n",
    "                found = True\n",
    "            \n",
    "    test_df = pd.concat(test_dfs)\n",
    "    drop_list = test_df['Unnamed: 0'].tolist()\n",
    "    train_df = df.copy().drop(drop_list)\n",
    "    \n",
    "    data_train = train_df[['isFirstCap', 'Length', 'endY', 'isNNP', 'isJJ', 'isCD', 'otherCap', 'endan',\n",
    "           'isNum', 'endS', 'endish', 'endese', 'propVow', 'frontWord', 'backWord']].values\n",
    "    label_train = train_df['TagNum'].values\n",
    "    data_test = test_df[['isFirstCap', 'Length', 'endY', 'isNNP', 'isJJ', 'isCD', 'otherCap', 'endan',\n",
    "           'isNum', 'endS', 'endish', 'endese', 'propVow', 'frontWord', 'backWord']].values\n",
    "    label_test = test_df['TagNum'].values\n",
    "    np.savetxt('data_train.txt', data_train)\n",
    "    np.savetxt('label_train.txt', label_train)\n",
    "    np.savetxt('data_test.txt', data_test)\n",
    "    np.savetxt('label_test.txt', label_test)\n",
    "split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
