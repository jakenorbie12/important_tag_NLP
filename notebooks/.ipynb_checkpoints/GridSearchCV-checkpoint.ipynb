{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "import nltk\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isFirstCap(x):\n",
    "    if x[0].isupper():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def Length(x):\n",
    "    return len(x)\n",
    "\n",
    "def endY(x):\n",
    "    if x[-1] == 'y':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def endan(x):\n",
    "    if x[-2:len(x)] == 'an':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def isNum(x):\n",
    "    if x.isnumeric():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def endS(x):\n",
    "    if x[-1] == 's':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def endish(x):\n",
    "    if x[-3:len(x)] == 'ish':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def endese(x):\n",
    "    if x[-3:len(x)] == 'ese':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def otherCap(x):\n",
    "    for letter in x:\n",
    "        if letter.isupper():\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def propVow(x):\n",
    "    vowels = 'aeiouAEIOU'\n",
    "    numVow = 0\n",
    "    for letter in x:\n",
    "        if letter in vowels:\n",
    "            numVow += 1\n",
    "    return numVow / len(x)\n",
    "\n",
    "def frontWord(x, array, df):\n",
    "    if x > 0:\n",
    "        return array.index(df['Word'][x-1])\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "def backWord(x, array, df):\n",
    "    if x < len(df.index) - 1:\n",
    "        return array.index(df['Word'][x+1])\n",
    "    else:\n",
    "        return\n",
    "\n",
    "def Array2Num(x, array):\n",
    "    return array.index(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Dataset_08-29-2019.txt'\n",
    "df = pd.read_csv(filename, sep = '\\t', encoding = 'unicode_escape')\n",
    "word_data = df['Word'].values\n",
    "word_vec = [nltk.word_tokenize(title) for title in word_data]\n",
    "model = Word2Vec(word_vec, size=24, window=5, min_count=0, workers=4)\n",
    "model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaken\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  import sys\n",
      "C:\\Users\\jaken\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "function_dict = {'isFirstCap': isFirstCap, 'Length': Length, 'endY': endY, 'otherCap': otherCap, 'endan': endan,\n",
    "                  'isNum': isNum, 'endS': endS, 'endish': endish, 'endese': endese, 'propVow': propVow}\n",
    "for f in function_dict:\n",
    "    df[f] = df['Word'].apply(lambda x: function_dict[f](x))\n",
    "wv = KeyedVectors.load('word2vec.model')\n",
    "for i in range(24):\n",
    "    df['WordVector' + str(i)] = df['Word'].apply(lambda x: wv[x][i] if x in wv else None)\n",
    "#df['WordVector'] = df['Word'].apply(lambda x: wv[x] if x in wv else None)\n",
    "tag_array = df.Tag.unique().tolist()\n",
    "df['TagNum'] = df['Tag'].apply(lambda x: Array2Num(x, tag_array))\n",
    "POS_array = df.POS.unique().tolist()\n",
    "df['POSNum'] = df['POS'].apply(lambda x: Array2Num(x, POS_array))\n",
    "word_array = df.Word.unique().tolist()\n",
    "df['frontWord'] = df['Unnamed: 0'].apply(lambda x: frontWord(x, word_array, df))\n",
    "df['backWord'] = df['Unnamed: 0'].apply(lambda x: backWord(x, word_array, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_group = df.groupby(['Sentence #'])\n",
    "test_sentences = []\n",
    "test_dfs = []\n",
    "train_dfs = []\n",
    "max_sent = int(df['Sentence #'].max())\n",
    "for i in range(round(max_sent / 4)):\n",
    "    found = False\n",
    "    while found == False:\n",
    "        num = random.randint(1, 2999)\n",
    "        if not num in test_sentences:\n",
    "            test_sentences.append(num)\n",
    "            test_dfs.append(sentences_group.get_group(num))\n",
    "            found = True\n",
    "            \n",
    "test_df = pd.concat(test_dfs)\n",
    "drop_list = test_df['Unnamed: 0'].tolist()\n",
    "train_df = df.copy().drop(drop_list)\n",
    "feature_list = ['isFirstCap', 'Length', 'endY', 'otherCap', 'endan',\n",
    "                   'isNum', 'endS', 'endish', 'endese', 'propVow', 'POSNum', 'frontWord', 'backWord']\n",
    "vectorlist = []\n",
    "for i in range(24):\n",
    "    vectorlist.append('WordVector' + str(i))\n",
    "feature_list = feature_list + vectorlist\n",
    "data_train = train_df[feature_list].values\n",
    "label_train = train_df['TagNum'].values\n",
    "data_test = test_df[feature_list].values\n",
    "label_test = test_df['TagNum'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, \n",
    "                       model, param_grid, cv=10, scoring_fit='neg_mean_squared_error',\n",
    "                       do_probabilities = False):\n",
    "    gs = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        n_jobs=-1, \n",
    "        scoring=scoring_fit,\n",
    "        verbose=2\n",
    "    )\n",
    "    fitted_model = gs.fit(X_train_data, y_train_data)\n",
    "    \n",
    "    if do_probabilities:\n",
    "      pred = fitted_model.predict_proba(X_test_data)\n",
    "    else:\n",
    "      pred = fitted_model.predict(X_test_data)\n",
    "    \n",
    "    return fitted_model, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2916 candidates, totalling 14580 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 19.5min\n",
      "C:\\Users\\jaken\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 48.7min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 101.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed: 179.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed: 258.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed: 339.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2584 tasks      | elapsed: 464.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3273 tasks      | elapsed: 557.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4042 tasks      | elapsed: 661.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4893 tasks      | elapsed: 786.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5824 tasks      | elapsed: 911.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6837 tasks      | elapsed: 1040.6min\n",
      "[Parallel(n_jobs=-1)]: Done 7930 tasks      | elapsed: 1219.7min\n",
      "[Parallel(n_jobs=-1)]: Done 9105 tasks      | elapsed: 1420.7min\n",
      "[Parallel(n_jobs=-1)]: Done 10360 tasks      | elapsed: 1628.3min\n",
      "[Parallel(n_jobs=-1)]: Done 11697 tasks      | elapsed: 1862.4min\n",
      "[Parallel(n_jobs=-1)]: Done 13114 tasks      | elapsed: 2099.5min\n",
      "[Parallel(n_jobs=-1)]: Done 14580 out of 14580 | elapsed: 2313.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9425158678082663\n",
      "{'colsample_bytree': 0.7, 'max_depth': 20, 'min_split_gain': 0.3, 'n_estimators': 400, 'num_leaves': 100, 'reg_alpha': 1.1, 'reg_lambda': 1.3, 'subsample': 0.9, 'subsample_freq': 20}\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier()\n",
    "param_grid = {\n",
    "    'n_estimators': [400, 700, 1000],\n",
    "    'colsample_bytree': [0.7, 0.8],\n",
    "    'max_depth': [15,20,25],\n",
    "    'num_leaves': [50, 100, 200],\n",
    "    'reg_alpha': [1.1, 1.2, 1.3],\n",
    "    'reg_lambda': [1.1, 1.2, 1.3],\n",
    "    'min_split_gain': [0.3, 0.4],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'subsample_freq': [20]\n",
    "}\n",
    "\n",
    "model, pred = algorithm_pipeline(data_train, data_test, label_train, label_test, model, \n",
    "                                 param_grid, cv=5, scoring_fit='accuracy')\n",
    "\n",
    "print(model.best_score_)\n",
    "print(model.best_params_)\n",
    "#{'colsample_bytree': 0.7, 'max_depth': 20, 'min_split_gain': 0.3, 'n_estimators': 400, 'num_leaves': 100, 'reg_alpha': 1.1, 'reg_lambda': 1.3, 'subsample': 0.9, 'subsample_freq': 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaken\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    }
   ],
   "source": [
    "d_train = lgb.Dataset(data_train, label=label_train)\n",
    "params = {'colsample_bytree': 0.7, 'max_depth': 20, 'min_split_gain': 0.3, 'n_estimators': 400, 'num_leaves': 100, 'reg_alpha': 1.1, 'reg_lambda': 1.3, 'subsample': 0.9, 'subsample_freq': 20}\n",
    "params['objective'] = 'multiclass'\n",
    "params['num_classes'] = 17\n",
    "params['learning_rate'] = 0.03\n",
    "mod = lgb.train(params, d_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9516872579752904\n",
      "[[13779     3     4    11     0    14     8    11     0     0    15     0\n",
      "      4     0     6     1     0]\n",
      " [   13   389    21    10     6    31     7     0     0     0    15     0\n",
      "      0     0     0     0     0]\n",
      " [    9    27   206     3     2     5     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [    6    17     0   187     0    11     3     1     1     0    41     0\n",
      "      0     0     0     0     0]\n",
      " [   10    14     1     0    49     0     7     0     0     0    14     1\n",
      "      0     0     0     0     0]\n",
      " [   21    60     9    15     0   186    11     0     1     0    12     0\n",
      "      0     0     1     0     0]\n",
      " [   29     8     0     7     8    17   138     0     0     1    38     0\n",
      "      0     0     0     0     0]\n",
      " [   45     1     0     0     0     1     0   243     0     0     0     0\n",
      "      9     0     0     0     0]\n",
      " [    3     3     1     2     0     3     0     0     0     0     2     0\n",
      "      0     0     0     0     0]\n",
      " [    4     0     0     0     0     0     0     0     0     0     3     0\n",
      "      0     0     0     0     0]\n",
      " [    9     2     0    20     1     3     6     0     0     0   261     0\n",
      "      0     0     0     0     0]\n",
      " [    1     0     1     0     0     0     0     0     0     0     1     0\n",
      "      0     0     0     0     0]\n",
      " [   39     0     0     0     0     0     0    19     0     0     3     0\n",
      "     31     0     0     0     0]\n",
      " [    1     1     0     0     0     0     0     0     0     0     0     0\n",
      "      0     2     0     0     0]\n",
      " [    0     3     0     0     0     0     0     0     0     0     1     0\n",
      "      0     0     6     1     0]\n",
      " [    0     0     0     1     0     0     0     0     0     0     4     0\n",
      "      1     0     0     4     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     2]]\n"
     ]
    }
   ],
   "source": [
    "prediction_data = mod.predict(data_test)\n",
    "classed_data = [np.argmax(line) for line in prediction_data]\n",
    "accuracy = accuracy_score(classed_data, label_test)\n",
    "cm = confusion_matrix(label_test, classed_data)\n",
    "print(accuracy)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
