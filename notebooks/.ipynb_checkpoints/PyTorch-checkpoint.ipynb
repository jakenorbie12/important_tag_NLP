{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary feature gen and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import random\n",
    "import torch\n",
    "#import nltk\n",
    "from torch import nn, optim\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isFirstCap(x):\n",
    "    if x[0].isupper():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def Length(x):\n",
    "    return len(x)\n",
    "\n",
    "def endY(x):\n",
    "    if x[-1] == 'y':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def endan(x):\n",
    "    if x[-2:len(x)] == 'an':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def isNum(x):\n",
    "    if x.isnumeric():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def endS(x):\n",
    "    if x[-1] == 's':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def endish(x):\n",
    "    if x[-3:len(x)] == 'ish':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def endese(x):\n",
    "    if x[-3:len(x)] == 'ese':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def otherCap(x):\n",
    "    for letter in x:\n",
    "        if letter.isupper():\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def propVow(x):\n",
    "    vowels = 'aeiouAEIOU'\n",
    "    numVow = 0\n",
    "    for letter in x:\n",
    "        if letter in vowels:\n",
    "            numVow += 1\n",
    "    return numVow / len(x)\n",
    "\n",
    "def frontWord(x, array, df):\n",
    "    if x > 0:\n",
    "        return array.index(df['Word'][x-1])\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def backWord(x, array, df):\n",
    "    if x < len(df.index) - 1:\n",
    "        return array.index(df['Word'][x+1])\n",
    "    else:\n",
    "        return -2\n",
    "\n",
    "def Array2Num(x, array):\n",
    "    return array.index(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Dataset_08-29-2019.txt'\n",
    "df = pd.read_csv(filename, sep = '\\t', encoding = 'unicode_escape')\n",
    "word_data = df['Word'].values.tolist()\n",
    "#word_vec = [nltk.word_tokenize(title) for title in word_data]\n",
    "size = len(word_data) \n",
    "idx_list = [idx + 1 for idx, val in\n",
    "            enumerate(word_data) if val == '.'] \n",
    "res = [word_data[i: j] for i, j in\n",
    "        zip([0] + idx_list, idx_list + \n",
    "        ([size] if idx_list[-1] != size else []))] \n",
    "model = Word2Vec(res, size=24, window=5, min_count=0, workers=4)\n",
    "model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaken\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "C:\\Users\\jaken\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "function_dict = {'isFirstCap': isFirstCap, 'Length': Length, 'endY': endY, 'otherCap': otherCap, 'endan': endan,\n",
    "                  'isNum': isNum, 'endS': endS, 'endish': endish, 'endese': endese, 'propVow': propVow}\n",
    "for f in function_dict:\n",
    "    df[f] = df['Word'].apply(lambda x: function_dict[f](x))\n",
    "wv = KeyedVectors.load('word2vec.model')\n",
    "\n",
    "\n",
    "\n",
    "#NEED TO FIX: WORD2VEC DOESN'T WORK WHEN SAVING TO CSV AND READING IT AGAIN\n",
    "\n",
    "\n",
    "\n",
    "for i in range(24):\n",
    "    df['WordVector' + str(i)] = df['Word'].apply(lambda x: wv[x][i] if x in wv else 0)\n",
    "#df['WordVector'] = df['Word'].apply(lambda x: wv[x] if x in wv else None)\n",
    "tag_array = df.Tag.unique().tolist()\n",
    "df['TagNum'] = df['Tag'].apply(lambda x: Array2Num(x, tag_array))\n",
    "POS_array = df.POS.unique().tolist()\n",
    "df['POSNum'] = df['POS'].apply(lambda x: Array2Num(x, POS_array))\n",
    "word_array = df.Word.unique().tolist()\n",
    "df['frontWord'] = df['Unnamed: 0'].apply(lambda x: frontWord(x, word_array, df))\n",
    "df['backWord'] = df['Unnamed: 0'].apply(lambda x: backWord(x, word_array, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_group = df.groupby(['Sentence #'])\n",
    "test_sentences = []\n",
    "test_dfs = []\n",
    "train_dfs = []\n",
    "max_sent = int(df['Sentence #'].max())\n",
    "for i in range(round(max_sent / 4)):\n",
    "    found = False\n",
    "    while found == False:\n",
    "        num = random.randint(1, max_sent)\n",
    "        if not num in test_sentences:\n",
    "            test_sentences.append(num)\n",
    "            test_dfs.append(sentences_group.get_group(num))\n",
    "            found = True\n",
    "            \n",
    "test_df = pd.concat(test_dfs)\n",
    "drop_list = test_df['Unnamed: 0'].tolist()\n",
    "train_df = df.copy().drop(drop_list)\n",
    "feature_list = ['isFirstCap', 'Length', 'endY', 'otherCap', 'endan',\n",
    "                   'isNum', 'endS', 'endish', 'endese', 'propVow', 'POSNum', 'frontWord', 'backWord']\n",
    "vectorlist = []\n",
    "for i in range(24):\n",
    "    vectorlist.append('WordVector' + str(i))\n",
    "feature_list = feature_list + vectorlist\n",
    "data_train = train_df[feature_list].values\n",
    "label_train = train_df['TagNum'].values\n",
    "data_test = test_df[feature_list].values\n",
    "label_test = test_df['TagNum'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(feature_list)\n",
    "input_size = 37\n",
    "hidden_sizes = [30, 23]\n",
    "#length of num_classes\n",
    "output_size = 17\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "    ('relu2', nn.ReLU()),\n",
    "    ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "    ('softmax', nn.Softmax(dim=1))\n",
    "]))\n",
    "\n",
    "'''\n",
    "multiplier_list = []\n",
    "for num in range(output_size):\n",
    "    multiplier_list.append(np.count_nonzero(label_train == num))\n",
    "'''\n",
    "weights = torch.tensor([1,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 17]) torch.Size([17])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (17).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-fea72195cc67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 948\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2420\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2421\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2422\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2214\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2215\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[1;32m-> 2216\u001b[1;33m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[0;32m   2217\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2218\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (17)."
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "x_factor = np.count_nonzero(label_train == 0)\n",
    "for i in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for idx, word in enumerate(data_train):\n",
    "        label = label_train[idx]\n",
    "        optimizer.zero_grad()\n",
    "        word = np.array([word])\n",
    "        word = torch.from_numpy(word)\n",
    "        output = model(word.float())\n",
    "        target_list = []\n",
    "        for i in range(output_size):\n",
    "            if i == label:\n",
    "                target_list.append(float(1))\n",
    "            else:\n",
    "                target_list.append(float(0))\n",
    "        target = torch.tensor(target_list)\n",
    "        print(output.size(), target.size())\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(data_train)}\")\n",
    "        \n",
    "correct = 0\n",
    "total = 0\n",
    "total_list = []\n",
    "for idx, guess in enumerate(data_test):\n",
    "    word = np.array([guess])\n",
    "    word = torch.from_numpy(word)\n",
    "    output = model(word.float())\n",
    "    guess_list = output[0].detach().numpy().tolist()\n",
    "    total_list.append(guess_list)\n",
    "\n",
    "\n",
    "classed_data = [np.argmax(line) for line in total_list]\n",
    "accuracy = accuracy_score(classed_data, label_test)\n",
    "cm = confusion_matrix(label_test, classed_data)\n",
    "print('Accuracy Score: ' + str(accuracy))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#TO FIX!!!! MODEL ONLY KICKS OUT 0'S\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lightgbm and normal project stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = lgb.Dataset(data_train, label=label_train)\n",
    "params = {}\n",
    "params['objective'] = 'multiclass'\n",
    "params['num_classes'] = 17\n",
    "params['learning_rate'] = 0.03\n",
    "mod = lgb.train(params, d_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9478391356542617\n",
      "[[14043     9     5     8     2     7     8     2     0     1    15     0\n",
      "      2     0     1     0     2]\n",
      " [   21   420    17     5     5    29     5     0     0     0    13     0\n",
      "      0     4     0     0     0]\n",
      " [   12    51   221     2     5     5     2     0     0     0     2     2\n",
      "      0     0     0     0     0]\n",
      " [   12    29     1   215     2    11     9     0     0     0    24     0\n",
      "      0     0     0     0     0]\n",
      " [   10     9     0     1    47     2     4     0     0     0    16     2\n",
      "      0     0     0     0     0]\n",
      " [   28    77     7    17     1   173     6     1     1     0    12     0\n",
      "      0     0     0     0     0]\n",
      " [   49    13     2    10     8     7   127     0     0     0    41     0\n",
      "      0     0     0     0     0]\n",
      " [   42     4     0     0     0     2     1   242     0     0     1     0\n",
      "      4     0     0     0     2]\n",
      " [    4     2     1     1     0     5     1     0     2     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [    3     1     0     1     1     1     3     0     0     0     4     0\n",
      "      0     0     0     0     0]\n",
      " [   16     4     0    24     4     1     9     0     0     0   271     0\n",
      "      0     0     2     0     0]\n",
      " [    1     1     0     0     3     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]\n",
      " [   34     1     0     0     0     0     0     5     0     0     0     0\n",
      "     18     0     0     0     0]\n",
      " [    2     0     0     2     0     1     0     0     0     0     0     0\n",
      "      0     1     0     0     0]\n",
      " [    2     5     0     1     0     2     0     0     0     0     0     0\n",
      "      0     0     5     0     0]\n",
      " [    2     0     0     1     0     1     1     0     0     0     2     0\n",
      "      0     0     1     5     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     3     0\n",
      "      0     0     0     0     1]]\n"
     ]
    }
   ],
   "source": [
    "prediction_data = mod.predict(data_test)\n",
    "classed_data = [np.argmax(line) for line in prediction_data]\n",
    "accuracy = accuracy_score(classed_data, label_test)\n",
    "cm = confusion_matrix(label_test, classed_data)\n",
    "print(accuracy)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
